{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ikig5Lx2PgJT"
      },
      "outputs": [],
      "source": [
        "%pip install langdetect\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split,cross_val_score,GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix,ConfusionMatrixDisplay\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer,TfidfVectorizer\n",
        "from wordcloud import WordCloud\n",
        "from langdetect import detect\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "sns.set()\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Wq8xVzQsOjVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mo-YMKvNMmtI"
      },
      "source": [
        "# Work With 'Job Description'\n",
        "Explore data and select feature to modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGnN2Bt5MmtJ"
      },
      "outputs": [],
      "source": [
        "jobpostDF = pd.read_csv('/content/drive/MyDrive/ColabData/job_postings.csv')\n",
        "jobpostDF.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6uN1V8Lo4aPa"
      },
      "outputs": [],
      "source": [
        "jobpostDF.isnull().sum()\n",
        "jobpostDF = jobpostDF.dropna(subset='description')\n",
        "jobpostDF = jobpostDF.loc[:,['job_id','title','description']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBz1EEi5MmtK"
      },
      "source": [
        "Build Text Cleaner with Various Method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pps2zjNh8Kj_"
      },
      "outputs": [],
      "source": [
        "contraction_mapping = {\n",
        "    \"ain't\": \"is not\",\n",
        "    \"aren't\": \"are not\",\n",
        "    \"can't\": \"cannot\",\n",
        "    \"'cause\": \"because\",\n",
        "    \"could've\": \"could have\",\n",
        "    \"couldn't\": \"could not\",\n",
        "    \"didn't\": \"did not\",\n",
        "    \"doesn't\": \"does not\",\n",
        "    \"don't\": \"do not\",\n",
        "    \"hadn't\": \"had not\",\n",
        "    \"hasn't\": \"has not\",\n",
        "    \"haven't\": \"have not\",\n",
        "    \"he'd\": \"he would\",\n",
        "    \"he'll\": \"he will\",\n",
        "    \"he's\": \"he is\",\n",
        "    \"how'd\": \"how did\",\n",
        "    \"how'd'y\": \"how do you\",\n",
        "    \"how'll\": \"how will\",\n",
        "    \"how's\": \"how is\",\n",
        "    \"I'd\": \"I would\",\n",
        "    \"I'd've\": \"I would have\",\n",
        "    \"I'll\": \"I will\",\n",
        "    \"I'll've\": \"I will have\",\n",
        "    \"I'm\": \"I am\",\n",
        "    \"I've\": \"I have\",\n",
        "    \"i'd\": \"i would\",\n",
        "    \"i'd've\": \"i would have\",\n",
        "    \"i'll\": \"i will\",\n",
        "    \"i'll've\": \"i will have\",\n",
        "    \"i'm\": \"i am\",\n",
        "    \"i've\": \"i have\",\n",
        "    \"isn't\": \"is not\",\n",
        "    \"it'd\": \"it would\",\n",
        "    \"it'd've\": \"it would have\",\n",
        "    \"it'll\": \"it will\",\n",
        "    \"it'll've\": \"it will have\",\n",
        "    \"it's\": \"it is\",\n",
        "    \"let's\": \"let us\",\n",
        "    \"ma'am\": \"madam\",\n",
        "    \"mayn't\": \"may not\",\n",
        "    \"might've\": \"might have\",\n",
        "    \"mightn't\": \"might not\",\n",
        "    \"mightn't've\": \"might not have\",\n",
        "    \"must've\": \"must have\",\n",
        "    \"mustn't\": \"must not\",\n",
        "    \"mustn't've\": \"must not have\",\n",
        "    \"needn't\": \"need not\",\n",
        "    \"needn't've\": \"need not have\",\n",
        "    \"o'clock\": \"of the clock\",\n",
        "    \"oughtn't\": \"ought not\",\n",
        "    \"oughtn't've\": \"ought not have\",\n",
        "    \"shan't\": \"shall not\",\n",
        "    \"sha'n't\": \"shall not\",\n",
        "    \"shan't've\": \"shall not have\",\n",
        "    \"she'd\": \"she would\",\n",
        "    \"she'd've\": \"she would have\",\n",
        "    \"she'll\": \"she will\",\n",
        "    \"she'll've\": \"she will have\",\n",
        "    \"she's\": \"she is\",\n",
        "    \"should've\": \"should have\",\n",
        "    \"shouldn't\": \"should not\",\n",
        "    \"shouldn't've\": \"should not have\",\n",
        "    \"so've\": \"so have\",\n",
        "    \"so's\": \"so as\",\n",
        "    \"this's\": \"this is\",\n",
        "    \"that'd\": \"that would\",\n",
        "    \"that'd've\": \"that would have\",\n",
        "    \"that's\": \"that is\",\n",
        "    \"there'd\": \"there would\",\n",
        "    \"there'd've\": \"there would have\",\n",
        "    \"there's\": \"there is\",\n",
        "    \"here's\": \"here is\",\n",
        "    \"they'd\": \"they would\",\n",
        "    \"they'd've\": \"they would have\",\n",
        "    \"they'll\": \"they will\",\n",
        "    \"they'll've\": \"they will have\",\n",
        "    \"they're\": \"they are\",\n",
        "    \"they've\": \"they have\",\n",
        "    \"to've\": \"to have\",\n",
        "    \"wasn't\": \"was not\",\n",
        "    \"we'd\": \"we would\",\n",
        "    \"we'd've\": \"we would have\",\n",
        "    \"we'll\": \"we will\",\n",
        "    \"we'll've\": \"we will have\",\n",
        "    \"we're\": \"we are\",\n",
        "    \"we've\": \"we have\",\n",
        "    \"weren't\": \"were not\",\n",
        "    \"what'll\": \"what will\",\n",
        "    \"what'll've\": \"what will have\",\n",
        "    \"what're\": \"what are\",\n",
        "    \"what's\": \"what is\",\n",
        "    \"what've\": \"what have\",\n",
        "    \"when's\": \"when is\",\n",
        "    \"when've\": \"when have\",\n",
        "    \"where'd\": \"where did\",\n",
        "    \"where's\": \"where is\",\n",
        "    \"where've\": \"where have\",\n",
        "    \"who'll\": \"who will\",\n",
        "    \"who'll've\": \"who will have\",\n",
        "    \"who's\": \"who is\",\n",
        "    \"who've\": \"who have\",\n",
        "    \"why's\": \"why is\",\n",
        "    \"why've\": \"why have\",\n",
        "    \"will've\": \"will have\",\n",
        "    \"won't\": \"will not\",\n",
        "    \"won't've\": \"will not have\",\n",
        "    \"would've\": \"would have\",\n",
        "    \"wouldn't\": \"would not\",\n",
        "    \"wouldn't've\": \"would not have\",\n",
        "    \"y'all\": \"you all\",\n",
        "    \"y'all'd\": \"you all would\",\n",
        "    \"y'all'd've\": \"you all would have\",\n",
        "    \"y'all're\": \"you all are\",\n",
        "    \"y'all've\": \"you all have\",\n",
        "    \"you'd\": \"you would\",\n",
        "    \"you'd've\": \"you would have\",\n",
        "    \"you'll\": \"you will\",\n",
        "    \"you'll've\": \"you will have\",\n",
        "    \"you're\": \"you are\",\n",
        "    \"you've\": \"you have\",\n",
        "    \"u.s\": \"america\",\n",
        "    \"e.g\": \"for example\",\n",
        "}\n",
        "\n",
        "\n",
        "# Clean contraction\n",
        "def clean_contractions(text):\n",
        "    specials = [\"’\", \"‘\", \"´\", \"`\"]\n",
        "    for s in specials:\n",
        "        text = text.replace(s, \"'\")\n",
        "    for word in contraction_mapping.keys():\n",
        "        if \"\" + word + \"\" in text:\n",
        "            text = text.replace(\"\" + word + \"\", \"\" + contraction_mapping[word] + \"\")\n",
        "    return text\n",
        "\n",
        "\n",
        "# Remove Url Pattern\n",
        "def remove_urls(text):\n",
        "    url_pattern = r\"https?://\\S+|www\\.\\S+\"\n",
        "    return re.sub(url_pattern, \"\", text)\n",
        "\n",
        "\n",
        "# Remove HTML Tag\n",
        "def remove_html(text):\n",
        "    html_pattern = re.compile(\"<.*?>\")\n",
        "    return html_pattern.sub(r\"\", text)\n",
        "\n",
        "\n",
        "# Remove special character \"!\"#$%&'()*+, -./:;<=>?@[\\]^_`{|}~\"\n",
        "def remove_punctuation(text):\n",
        "    punctuation = string.punctuation + \"–\"\n",
        "    return re.sub(f\"[{re.escape(punctuation)}]\", \"\", text)\n",
        "\n",
        "\n",
        "# Remove E-mail pattern\n",
        "def remove_emails(text):\n",
        "    return re.sub(r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b\", \"\", text)\n",
        "\n",
        "\n",
        "# Remove New Line Code Snippet\n",
        "def remove_code_snippet(text):\n",
        "    return text.replace(\"\\n\", \"\")\n",
        "\n",
        "\n",
        "# Remove Emoji\n",
        "def remove_emoji(string):\n",
        "    emoji_pattern = re.compile(\n",
        "        \"[\"\n",
        "        \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        \"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "        \"\\U00002702-\\U000027B0\"\n",
        "        \"\\U000024C2-\\U0001F251\"\n",
        "        \"]+\",\n",
        "        flags=re.UNICODE,\n",
        "    )\n",
        "    return emoji_pattern.sub(r\"\", string)\n",
        "\n",
        "#Remove Non-English from Text\n",
        "def remove_non_english_text(text):\n",
        "    def is_english(text):\n",
        "        try:\n",
        "            return detect(text) == \"en\"\n",
        "        except:\n",
        "            return False\n",
        "\n",
        "    return text if is_english(text) else \"\"\n",
        "\n",
        "\n",
        "# Remove Digits\n",
        "def remove_digits(text):\n",
        "    return \"\".join(filter(lambda char: not char.isdigit(), text))\n",
        "\n",
        "\n",
        "# Remove Stop Words\n",
        "def remove_stopwords(text):\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    custom_stop_words = [\n",
        "        \"job\",\n",
        "        \"role\",\n",
        "        \"position\",\n",
        "        \"responsibility\",\n",
        "        \"responsibilities\",\n",
        "        \"duties\",\n",
        "        \"duty\",\n",
        "        \"requirement\",\n",
        "        \"requirements\",\n",
        "        \"qualification\",\n",
        "        \"qualifications\",\n",
        "        \"description\",\n",
        "        \"descriptions\",\n",
        "        \"candidate\",\n",
        "        \"candidates\",\n",
        "        \"applicant\",\n",
        "        \"applicants\",\n",
        "        \"opportunity\",\n",
        "        \"opportunities\",\n",
        "        \"team\",\n",
        "        \"teams\",\n",
        "        \"work\",\n",
        "        \"working\",\n",
        "        \"employee\",\n",
        "        \"employees\",\n",
        "        \"employer\",\n",
        "        \"employers\",\n",
        "        \"company\",\n",
        "        \"companies\",\n",
        "        \"location\",\n",
        "        \"locations\",\n",
        "        \"department\",\n",
        "        \"departments\",\n",
        "        \"report\",\n",
        "        \"reports\",\n",
        "        \"reporting\",\n",
        "        \"benefit\",\n",
        "        \"benefits\",\n",
        "        \"compensation\",\n",
        "        \"salary\",\n",
        "        \"experience\",\n",
        "        \"experienced\",\n",
        "        \"year\",\n",
        "        \"years\",\n",
        "        \"gender\",\n",
        "        \"race\",\n",
        "        \"color\",\n",
        "        \"sex\",\n",
        "        \"orientation\",\n",
        "        \"sexual\",\n",
        "        \"religion\",\n",
        "        \"national\",\n",
        "        \"identify\",\n",
        "        \"veteran\",\n",
        "        \"nation\",\n",
        "        \"including\",\n",
        "        \"required\",\n",
        "        \"disability\",\n",
        "        \"regard\"\n",
        "    ]\n",
        "\n",
        "    words = text.split()\n",
        "    filtered_words = [\n",
        "        word\n",
        "        for word in words\n",
        "        if word.lower() not in stop_words and word.lower() not in custom_stop_words\n",
        "    ]\n",
        "    return \" \".join(filtered_words)\n",
        "\n",
        "\n",
        "# Lemmatization\n",
        "def lemmatize_words(text):\n",
        "    words = text.split()\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word, pos='v') for word in words]\n",
        "    return \" \".join(lemmatized_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wx5RB4NOMmtQ"
      },
      "outputs": [],
      "source": [
        "def clean_text(text,remove_stop_words=True):\n",
        "    text = text.lower()\n",
        "    text = remove_urls(text)\n",
        "    text = remove_emails(text)\n",
        "    text = remove_html(text)\n",
        "    text = clean_contractions(text)\n",
        "    text = remove_code_snippet(text)\n",
        "    text = remove_punctuation(text)\n",
        "    text = remove_emoji(text)\n",
        "    text = remove_digits(text)\n",
        "    text = remove_non_english_text(text)\n",
        "    text = lemmatize_words(text)\n",
        "    if remove_stop_words:\n",
        "        text = remove_stopwords(text)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqBBmoOZMmtQ"
      },
      "source": [
        "Let's Clean 'Job Description'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EImjbhifMmtQ"
      },
      "outputs": [],
      "source": [
        "#Clean Job description text\n",
        "jobpostDF['description_cleaned'] = jobpostDF['description'].astype(str).apply(lambda x: clean_text(x))\n",
        "jobpostDF['description_cleaned_st'] = jobpostDF['description'].astype(str).apply(lambda x: clean_text(x, remove_stop_words=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_U_89cAhMmtQ"
      },
      "source": [
        "Collect **most common** word that can occur in every job description and also collect **rare word** then remove it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xlnTeT2MmtQ"
      },
      "outputs": [],
      "source": [
        "cnt = Counter()\n",
        "for text in jobpostDF[\"description_cleaned\"].values:\n",
        "    for word in text.split():\n",
        "        cnt[word] += 1\n",
        "freqWords = set([w[0] for w in cnt.most_common(10)])\n",
        "rareWords = set([w for w, freq in cnt.items() if freq == 1])\n",
        "\n",
        "def remove_freq_rare_words(text):\n",
        "    preserved_words = {'management', 'product', 'project'}\n",
        "    return \" \".join([word for word in str(text).split() if word not in freqWords and word not in rareWords or word in preserved_words])\n",
        "jobpostDF['description_cleaned'] = jobpostDF['description_cleaned'].apply(remove_freq_rare_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwwquxYlMmtR"
      },
      "source": [
        "Count 'Job Description' Length and Remove text that have 0,1,2 length because it not make sense and can be empty text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gMbOOun_MmtR"
      },
      "outputs": [],
      "source": [
        "#Count Text Lenght\n",
        "#Remove Description that have 0 and 1 length\n",
        "jobpostDF['original_length'] = jobpostDF['description'].str.split().apply(len)\n",
        "jobpostDF['cleaned_length'] = jobpostDF['description_cleaned'].str.split().apply(len)\n",
        "jobpostDF = jobpostDF[jobpostDF['cleaned_length'].isin([0,1,2])==False]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4rqmeYjMmtR"
      },
      "source": [
        "# Visualization of Job Description Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SR0Ty8ZMmtR"
      },
      "source": [
        "##### Distribution of Text Length for Job Description"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-bh-5gzMmtR"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(16,7), facecolor='none')\n",
        "\n",
        "ax1 = fig.add_subplot(121)\n",
        "sns.histplot(jobpostDF['original_length'], ax=ax1, color='blue', bins=30, zorder=1)\n",
        "ax1.set_title('Original Descriptions')\n",
        "ax1.set_facecolor('none')\n",
        "\n",
        "\n",
        "ax2 = fig.add_subplot(122)\n",
        "sns.histplot(jobpostDF['cleaned_length'], ax=ax2, color='green', bins=30, zorder=1)\n",
        "ax2.set_title('Cleaned Descriptions')\n",
        "ax2.set_facecolor('none')\n",
        "\n",
        "\n",
        "describe_original = jobpostDF.original_length.describe().to_frame().round(2)\n",
        "bbox_original = [0.65, 0.55, 0.3, 0.4]\n",
        "table_original = ax1.table(cellText=describe_original.values, rowLabels=describe_original.index, bbox=bbox_original, colLabels=describe_original.columns, zorder=2)\n",
        "table_original.auto_set_font_size(False)\n",
        "table_original.set_fontsize(12)\n",
        "# table_original.auto_set_column_width(col=list(range(len(describe_original.columns))))\n",
        "for key, cell in table_original.get_celld().items():\n",
        "    cell.set_text_props(ha='center', va='center')\n",
        "    cell.set_height(0.2)\n",
        "\n",
        "\n",
        "describe_cleaned = jobpostDF.cleaned_length.describe().to_frame().round(2)\n",
        "bbox_cleaned = [0.65, 0.55, 0.3, 0.4]\n",
        "table_cleaned = ax2.table(cellText=describe_cleaned.values, rowLabels=describe_cleaned.index, bbox=bbox_cleaned, colLabels=describe_cleaned.columns, zorder=2)\n",
        "table_cleaned.auto_set_font_size(False)\n",
        "table_cleaned.set_fontsize(12)\n",
        "# table_cleaned.auto_set_column_width(col=list(range(len(describe_cleaned.columns))))\n",
        "for key, cell in table_cleaned.get_celld().items():\n",
        "    cell.set_text_props(ha='center', va='center')\n",
        "    cell.set_height(0.2)\n",
        "\n",
        "fig.suptitle('Distribution of Text Length for Job Description: Before vs. After Cleaning', fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.savefig('transparent_histogram.png', transparent=True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJod0P-TMmtR"
      },
      "source": [
        "##### Word Clound Before and After Clean Process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x94j1GQbMmtR"
      },
      "outputs": [],
      "source": [
        "unclean = ' '.join([text for text in jobpostDF['description']])\n",
        "clean = ' '.join([text for text in jobpostDF['description_cleaned']])\n",
        "\n",
        "# Generate word clouds\n",
        "wordcloud_unclean = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(unclean)\n",
        "wordcloud_clean = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(clean)\n",
        "\n",
        "plt.figure(figsize=(24, 9))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(wordcloud_unclean, interpolation=\"bilinear\")\n",
        "plt.title(\"Unclean Text\", fontsize = 20)\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(wordcloud_clean, interpolation=\"bilinear\")\n",
        "plt.title(\"Cleaned Text\", fontsize = 20)\n",
        "plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoDSu94qMmtR"
      },
      "source": [
        "##### Top 10 Word in Job Description"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5KSrSeOgMmtR"
      },
      "outputs": [],
      "source": [
        "cnt = Counter()\n",
        "for text in jobpostDF['description_cleaned'].values:\n",
        "    for word in text.split():\n",
        "        cnt[word] += 1\n",
        "\n",
        "mostCommon = cnt.most_common(10)\n",
        "\n",
        "words = []\n",
        "freq = []\n",
        "for word, count in mostCommon:\n",
        "    words.append(word)\n",
        "    freq.append(count)\n",
        "\n",
        "sns.barplot(x=freq, y=words)\n",
        "plt.title('Top 10 Most Frequently Occuring Words')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9gWe91vMmtR"
      },
      "source": [
        "##### Plot Unigrams , Bigrams and Trigrams in 'Job Description' before and after remove 'Stop Word\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzPI5HxgMmtR"
      },
      "outputs": [],
      "source": [
        "def get_top_ngrams(corpus, ngram_range, n=None):\n",
        "    vec = CountVectorizer(ngram_range=ngram_range).fit(corpus)\n",
        "    bag_of_words = vec.transform(corpus)\n",
        "\n",
        "    sum_words = bag_of_words.sum(axis=0)\n",
        "\n",
        "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
        "    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    common_words = words_freq[:n]\n",
        "    words = []\n",
        "    freqs = []\n",
        "    for word, freq in common_words:\n",
        "        words.append(word)\n",
        "        freqs.append(freq)\n",
        "\n",
        "    df = pd.DataFrame({'Word': words, 'Freq': freqs})\n",
        "    return df\n",
        "#collect n-gram for job description without Stop Word\n",
        "unigrams = get_top_ngrams(jobpostDF['description_cleaned'], (1, 1),20)\n",
        "bigrams = get_top_ngrams(jobpostDF['description_cleaned'], (2, 2), 20)\n",
        "trigrams = get_top_ngrams(jobpostDF['description_cleaned'], (3, 3),20)\n",
        "#collect n-gram for job description with Stop Word\n",
        "unigrams_st = get_top_ngrams(jobpostDF['description_cleaned_st'], (1, 1),20)\n",
        "bigrams_st = get_top_ngrams(jobpostDF['description_cleaned_st'], (2, 2),20)\n",
        "trigrams_st = get_top_ngrams(jobpostDF['description_cleaned_st'], (3, 3),20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysgVFJ4kMmtR"
      },
      "source": [
        "N-Gram before remove Stop Words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjsLqy2aMmtR"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(24, 12))\n",
        "\n",
        "plt.subplot(1,3,1)\n",
        "sns.barplot(x='Freq', y='Word', data=unigrams_st)\n",
        "plt.title('Top 20 Unigrams before removing stopwords', size=15)\n",
        "\n",
        "plt.subplot(1,3,2)\n",
        "sns.barplot(x='Freq', y='Word', data=bigrams_st)\n",
        "plt.title('Top 20 Bigrams before removing stopwords', size=15)\n",
        "\n",
        "plt.subplot(1,3,3)\n",
        "sns.barplot(x='Freq', y='Word', data=trigrams_st)\n",
        "plt.title('Top 20 Trigrams before removing stopwords', size=15)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbExwm54MmtR"
      },
      "source": [
        "N-Gram before after Stop Words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6NqgPpQoMmtR"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(24, 12))\n",
        "plt.subplot(1,3,1)\n",
        "sns.barplot(x='Freq', y='Word', data=unigrams)\n",
        "plt.title('Top 20 Unigrams after removing stopwords', size=15)\n",
        "\n",
        "plt.subplot(1,3,2)\n",
        "sns.barplot(x='Freq', y='Word', data=bigrams)\n",
        "plt.title('Top 20 Bigrams after removing stopwords', size=15)\n",
        "\n",
        "plt.subplot(1,3,3)\n",
        "sns.barplot(x='Freq', y='Word', data=trigrams)\n",
        "plt.title('Top 20 Trigrams after removing stopwords', size=15)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cx1YexCHMmtS"
      },
      "source": [
        "N-Gram Count in Job Description"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bd7bfkc2MmtS"
      },
      "outputs": [],
      "source": [
        "bow_converter = CountVectorizer(tokenizer=lambda doc: doc, lowercase=False, token_pattern=None)\n",
        "x = bow_converter.fit_transform(jobpostDF['description_cleaned'])\n",
        "words = bow_converter.get_feature_names_out()\n",
        "\n",
        "bigram_converter = CountVectorizer(tokenizer=lambda doc: doc, ngram_range=(2,2), lowercase=False, token_pattern=None)\n",
        "x2 = bigram_converter.fit_transform(jobpostDF['description_cleaned'])\n",
        "bigrams = bigram_converter.get_feature_names_out()\n",
        "\n",
        "trigram_converter = CountVectorizer(tokenizer=lambda doc: doc, ngram_range=(3,3), lowercase=False, token_pattern=None)\n",
        "x3 = trigram_converter.fit_transform(jobpostDF['description_cleaned'])\n",
        "trigrams = trigram_converter.get_feature_names_out()\n",
        "\n",
        "sns.set_style(\"white\")\n",
        "counts = [len(words), len(bigrams), len(trigrams)]\n",
        "plt.plot(counts, color='blue')\n",
        "plt.plot(counts, 'bo')\n",
        "plt.ticklabel_format(style = 'plain')\n",
        "plt.xticks(range(3), ['unigram', 'bigram', 'trigram'])\n",
        "plt.tick_params(labelsize=14)\n",
        "plt.title('Number of ngrams in Job Description', {'fontsize':16})\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6N4ZmwGvMmtS"
      },
      "source": [
        "# Work with Job Skill Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PdffEvzVMmtS"
      },
      "outputs": [],
      "source": [
        "jobskillsDF = pd.read_csv('job_skills.csv')\n",
        "jobskillsDF.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2LiNQzdyMmtS"
      },
      "outputs": [],
      "source": [
        "jobskillsDF.isnull().sum()\n",
        "jobskillsDF = jobskillsDF.dropna(subset='skill_abr')\n",
        "jobskillsDF.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NHjC5xKMmtS"
      },
      "source": [
        "Take a Closer Look for How Many 'Job Skill' In This Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59Z25CQyMmtS"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6), facecolor='none')\n",
        "ax = sns.countplot(x=jobskillsDF['skill_abr'], width=0.6)\n",
        "\n",
        "palette = sns.color_palette(\"deep\", len(ax.patches))\n",
        "for bar, color in zip(ax.patches, palette):\n",
        "    bar.set_color(color)\n",
        "\n",
        "for p in ax.patches:\n",
        "    ax.annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                ha='center', va='center', fontsize=8, color='black', xytext=(0, 5),\n",
        "                textcoords='offset points')\n",
        "\n",
        "unique_skills = jobskillsDF['skill_abr'].nunique()\n",
        "ax.text(0.95, 0.95, f'Unique Skills : {unique_skills}', transform=ax.transAxes,\n",
        "        verticalalignment='top', horizontalalignment='right', fontsize=20, color='Black')\n",
        "\n",
        "unique_jobs = jobskillsDF['job_id'].nunique()\n",
        "ax.text(0.95, 0.85, f'Jobs : {unique_jobs}', transform=ax.transAxes,\n",
        "        verticalalignment='top', horizontalalignment='right', fontsize=20, color='Black')\n",
        "\n",
        "ax.set_facecolor('none')\n",
        "\n",
        "plt.title('Count of Job Skills', fontsize=16)\n",
        "plt.xticks(rotation=45, fontsize=8)\n",
        "ax.set_xlabel(\"Skills\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2p1u4cLMmtS"
      },
      "source": [
        "Regroup with Related Skill to Only 10 Skill + 1 Other Skill"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9jxWqt-E5Q_0"
      },
      "outputs": [],
      "source": [
        "#Grouping Skill to 10 Skill\n",
        "skill_mapping = { 'ADM': 'ADM', #1.Administration\n",
        "                 'CNSL': 'ADM',\n",
        "                  'HR': 'ADM',\n",
        "                  'LGL': 'ADM',\n",
        "                  'MGMT': 'ADM',\n",
        "                  'PRJM':'ADM',\n",
        "                  'ACCT':'FIN', #2.Business and Finace\n",
        "                  'CUST':'FIN',\n",
        "                  'DIST':'FIN',\n",
        "                  'FIN':'FIN',\n",
        "                  'PRCH':'FIN',\n",
        "                  'SALE':'FIN',\n",
        "                  'STRA':'FIN',\n",
        "                  'SUPL':'FIN',\n",
        "                  'BD':'FIN',\n",
        "                  'GENB':'FIN',\n",
        "                  'ART':'DSGN', #3.Creative and Design\n",
        "                  'DSGN':'DSGN',\n",
        "                  'WRT':'DSGN',\n",
        "                  'EDU':'EDU', #4.Education\n",
        "                  'TRNG':'EDU',\n",
        "                  'ENG':'ENG',#5.Engineering\n",
        "                  'IT':'ENG',\n",
        "                  'MNFC':'ENG',\n",
        "                  'HCPR':'HCPR',#6.Healthcare\n",
        "                  'ADVR':'MRKT',#7.Marketing and Advertising\n",
        "                  'MRKT':'MRKT',\n",
        "                  'PR':'MRKT',\n",
        "                  'PRDM':'PRDM',#8.Product Development\n",
        "                  'ANLS':'RSCH',#9.Research and Science\n",
        "                  'SCI':'RSCH',\n",
        "                  'RSCH':'RSCH',\n",
        "                  'QA':'RSCH',\n",
        "                  'PROD':'PROD'}#10.Project Management\n",
        "jobskillsDF['skill_abr_regroup'] = jobskillsDF['skill_abr'].replace(skill_mapping)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lyRrNN-4MmtS"
      },
      "outputs": [],
      "source": [
        "#Remove duplicate row because 1 job may have multi skill but after grouping skill it can be the same skill\n",
        "jobskillsDF = jobskillsDF.drop_duplicates(subset=['job_id', 'skill_abr_regroup'])\n",
        "print(jobskillsDF.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FE3pV0tIMmtS"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6), facecolor='none')\n",
        "ax = sns.countplot(x=jobskillsDF['skill_abr_regroup'], width=0.6)\n",
        "\n",
        "palette = sns.color_palette(\"deep\", len(ax.patches))\n",
        "for bar, color in zip(ax.patches, palette):\n",
        "    bar.set_color(color)\n",
        "\n",
        "for p in ax.patches:\n",
        "    ax.annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                ha='center', va='center', fontsize=8, color='black', xytext=(0, 5),\n",
        "                textcoords='offset points')\n",
        "\n",
        "unique_skills_regroup = jobskillsDF['skill_abr_regroup'].nunique()\n",
        "ax.text(0.95, 0.95, f'Unique Skills : {unique_skills_regroup}', transform=ax.transAxes,\n",
        "        verticalalignment='top', horizontalalignment='right', fontsize=20, color='Black')\n",
        "\n",
        "unique_jobs_regroup = jobskillsDF['job_id'].nunique()\n",
        "ax.text(0.95, 0.85, f'Jobs : {unique_jobs_regroup}', transform=ax.transAxes,\n",
        "        verticalalignment='top', horizontalalignment='right', fontsize=20, color='Black')\n",
        "\n",
        "ax.set_facecolor('none')  # Set facecolor to 'none'\n",
        "\n",
        "plt.title('Count of Job Skills', fontsize=16)\n",
        "plt.xticks(rotation=45, fontsize=8)\n",
        "ax.set_xlabel(\"Skills\")\n",
        "plt.tight_layout()\n",
        "plt.savefig('transparent_countplot_regroup.png', transparent=True)  # Save with transparent background\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8pZ3suRPrgX"
      },
      "source": [
        "# Merge Job Description with Job Skills"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SRTUfgfMmtS"
      },
      "source": [
        "- Create Multi-Label for Job Skill (1 Job : Multi Skill)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lsgrdHsxPpUh"
      },
      "outputs": [],
      "source": [
        "multiskillDF = pd.merge(jobpostDF, jobskillsDF, on='job_id', how='inner') # merge skill type with description mathc by job id\n",
        "multiskillDF = multiskillDF.groupby('job_id').agg({'title': 'first', 'description_cleaned': 'first', 'skill_abr_regroup': ','.join}).reset_index()\n",
        "multiskillDF['skill_count'] = multiskillDF['skill_abr_regroup'].str.split(',').apply(len)\n",
        "multiskillDF['skill_abr_regroup'] = multiskillDF['skill_abr_regroup'].str.split(',').tolist()\n",
        "multiskillDF.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fawyKoN9MmtW"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "ax = sns.countplot(x=multiskillDF['skill_count'], width=0.6)\n",
        "\n",
        "palette = sns.color_palette(\"pastel\", len(ax.patches))\n",
        "for bar, color in zip(ax.patches, palette):\n",
        "    bar.set_color(color)\n",
        "\n",
        "for p in ax.patches:\n",
        "    ax.annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                ha='center', va='center', fontsize=14, color='black', xytext=(0, 5),\n",
        "                textcoords='offset points')\n",
        "\n",
        "labels = [f\"{int(label) + 1} Skills\" for label in ax.get_xticks()]\n",
        "ax.set_xticklabels(labels)\n",
        "\n",
        "plt.title('Count of Job Skills', fontsize=16)\n",
        "plt.xticks(rotation=0, fontsize=12)\n",
        "ax.set_xlabel(\"Number of Skills required\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5uaz8ONMmtW"
      },
      "source": [
        "- Creat Single Label for Job Skill Dataframe to Meansure TF-IDF Score of Each Job Skill"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aAVEguzEMmtW"
      },
      "outputs": [],
      "source": [
        "oneskillDF = multiskillDF[multiskillDF['skill_count'] == 1].copy()\n",
        "oneskillDF['skill_abr_regroup'] = oneskillDF['skill_abr_regroup'].str.join('')\n",
        "print(oneskillDF.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PsamHLDDMmtW"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "ax = sns.countplot(x=oneskillDF['skill_abr_regroup'], width=0.6)\n",
        "\n",
        "palette = sns.color_palette(\"deep\", len(ax.patches))\n",
        "for bar, color in zip(ax.patches, palette):\n",
        "    bar.set_color(color)\n",
        "\n",
        "for p in ax.patches:\n",
        "    ax.annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                ha='center', va='center', fontsize=10, color='black', xytext=(0, 5),\n",
        "                textcoords='offset points')\n",
        "\n",
        "nu_skill = oneskillDF['skill_abr_regroup'].nunique()\n",
        "ax.text(0.95, 0.95, f'Unique Skills : {nu_skill}', transform=ax.transAxes,\n",
        "        verticalalignment='top', horizontalalignment='right', fontsize=20, color='Black')\n",
        "\n",
        "jobs = oneskillDF['job_id'].nunique()\n",
        "ax.text(0.95, 0.85, f'Jobs : {jobs}', transform=ax.transAxes,\n",
        "        verticalalignment='top', horizontalalignment='right', fontsize=20, color='Black')\n",
        "\n",
        "plt.title('Count of Job Skills', fontsize=16)\n",
        "plt.xticks(rotation=45, fontsize=8)\n",
        "ax.set_xlabel(\"Skills\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qzyCn1ZMmtW"
      },
      "source": [
        "### TF-IDF Score of Job Description and Skills"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKdn6l-HMmtW"
      },
      "outputs": [],
      "source": [
        "tfidf_vectorizor = TfidfVectorizer(min_df=5,\n",
        "                             max_df=0.5,\n",
        "                             analyzer='word',\n",
        "                             strip_accents='unicode',\n",
        "                             ngram_range=(1, 2),\n",
        "                             sublinear_tf=True,\n",
        "                             smooth_idf=True,\n",
        "                             use_idf=True)\n",
        "def top_tfidf_feats(row, features, top_n=20):\n",
        "    topn_ids = np.argsort(row)[::-1][:top_n]\n",
        "    top_feats = [(features[i], row[i]) for i in topn_ids]\n",
        "    df = pd.DataFrame(top_feats)\n",
        "    df.columns = ['feature', 'tfidf']\n",
        "    return df\n",
        "\n",
        "def top_feats_in_doc(Xtr, features, row_id, top_n=20):\n",
        "    row = np.squeeze(Xtr[row_id].toarray())\n",
        "    return top_tfidf_feats(row, features, top_n)\n",
        "\n",
        "def top_mean_feats(Xtr, features, grp_ids=None, min_tfidf=0.1, top_n=10):\n",
        "    if grp_ids:\n",
        "        D = Xtr[grp_ids].toarray()\n",
        "    else:\n",
        "        D = Xtr.toarray()\n",
        "\n",
        "    D[D < min_tfidf] = 0\n",
        "    tfidf_means = np.mean(D, axis=0)\n",
        "    return top_tfidf_feats(tfidf_means, features, top_n)\n",
        "\n",
        "def top_feats_by_class(Xtr, y, features, min_tfidf=0.1, top_n=16):\n",
        "    dfs = []\n",
        "    labels = np.unique(y)\n",
        "    for label in labels:\n",
        "        ids = np.where(y==label)\n",
        "        feats_df = top_mean_feats(Xtr, features, ids, min_tfidf=min_tfidf, top_n=top_n)\n",
        "        feats_df.label = label\n",
        "        dfs.append(feats_df)\n",
        "    return dfs\n",
        "\n",
        "def plot_tfidf_classfeats_h(dfs, num_class=9):\n",
        "    num_class = len(dfs)\n",
        "    fig = plt.figure(figsize=(12, num_class*10), facecolor='none')\n",
        "    x = np.arange(len(dfs[0]))\n",
        "    for i, df in enumerate(dfs):\n",
        "        ax = fig.add_subplot(num_class, 1, i+1)\n",
        "        ax.spines[\"top\"].set_visible(False)\n",
        "        ax.spines[\"right\"].set_visible(False)\n",
        "        ax.set_frame_on(False)\n",
        "        ax.get_xaxis().tick_bottom()\n",
        "        ax.get_yaxis().tick_left()\n",
        "        ax.set_xlabel(\"Mean Tf-Idf Score\", labelpad=16, fontsize=16)\n",
        "        ax.set_ylabel(\"Word\", labelpad=16, fontsize=16)\n",
        "        ax.set_title(str(df.label) + ' Skill', fontsize=25)\n",
        "        ax.ticklabel_format(axis='x', style='sci', scilimits=(-2,2))\n",
        "        ax.barh(x, df.tfidf, align='center')\n",
        "        ax.set_yticks(x)\n",
        "        ax.set_ylim([-1, x[-1]+1])\n",
        "        ax.invert_yaxis()\n",
        "        yticks = ax.set_yticklabels(df.feature)\n",
        "        ax.set_facecolor('none')\n",
        "\n",
        "        for tick in ax.yaxis.get_major_ticks():\n",
        "                tick.label1.set_fontsize(20)\n",
        "        plt.subplots_adjust(bottom=0.09, right=0.97, left=0.15, top=0.95, wspace=0.52)\n",
        "    plt.savefig('transparent_tfidf_plot.png', transparent=True,dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "tfidf_vectorizor.fit(list(oneskillDF['description_cleaned']))\n",
        "\n",
        "class_Xtr = tfidf_vectorizor.transform(oneskillDF['description_cleaned'])\n",
        "class_y = oneskillDF['skill_abr_regroup']\n",
        "class_features = tfidf_vectorizor.get_feature_names_out()\n",
        "class_top_dfs = top_feats_by_class(class_Xtr, class_y, class_features)\n",
        "plot_tfidf_classfeats_h(class_top_dfs, 10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eyC8qaYfVyJ"
      },
      "source": [
        "# Train and Test data set with Bag Of Word and TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aF9Agz59MmtW"
      },
      "outputs": [],
      "source": [
        "count_vect = CountVectorizer(ngram_range=(1, 2))\n",
        "transformer = TfidfTransformer(norm='l2',sublinear_tf=True)\n",
        "mlb = MultiLabelBinarizer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjIYpzLKMmtW"
      },
      "source": [
        "## Building and evaluating a model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhXJKNn8MmtW"
      },
      "source": [
        "Tuning Hyperparameter for ***Navie Bayes with TF-IDF method***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ytGShiCOMmtW"
      },
      "outputs": [],
      "source": [
        "# mnb_model = MultiOutputClassifier(MultinomialNB())\n",
        "# alpha = [0.1,0.3,0.5]\n",
        "# paramgrid = {'estimator__alpha':alpha}\n",
        "# gsearch_cv = GridSearchCV(mnb_model, param_grid=paramgrid, cv=5)\n",
        "# gsearch_cv.fit(x_train_tfidf, y_train)\n",
        "\n",
        "# best_alpha = gsearch_cv.best_params_['estimator__alpha']\n",
        "# print(f\"Best alpha: {best_alpha}\")\n",
        "\n",
        "# mean_test_scores = gsearch_cv.cv_results_['mean_test_score']\n",
        "# plt.plot(alpha, mean_test_scores, marker='o')\n",
        "# plt.xlabel('Alpha')\n",
        "# plt.ylabel('Mean Test Score (Accuracy)')\n",
        "# plt.title('Alpha vs. Mean Test Score')\n",
        "# plt.grid(True)\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2pJQuCfMmtW"
      },
      "source": [
        "After Tuning HyperParameter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zQRJoijMmtW"
      },
      "source": [
        "Logistic Regression + One Vs Rest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEj2l256MmtX"
      },
      "outputs": [],
      "source": [
        "def LR_classify(X_tr, y_tr, X_test, y_test, description):\n",
        "    model = OneVsRestClassifier(LogisticRegression(max_iter=500)).fit(X_tr, y_tr)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    #Classification Report\n",
        "    clf_report = classification_report(y_test, y_pred, output_dict=True, zero_division=1)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.heatmap(pd.DataFrame(clf_report).iloc[:-1, :].T, annot=True)\n",
        "    plt.title(\"Logistic Regression Classification ({}) Accuracy Rate: {:.2f}%\".format(description, accuracy_score(y_test, y_pred)*100))\n",
        "    plt.show()\n",
        "\n",
        "    scores = cross_val_score(model, X_tr, y_tr, cv=5)\n",
        "    print(\"Accuracy: {:.2f}% (+/- {:.2f}%)\".format(scores.mean()*100, scores.std()*200))\n",
        "\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(scores, '-o', label='Cross-Validation Scores')\n",
        "    plt.axhline(y=scores.mean(), color='r', linestyle='--', label='Mean Score')\n",
        "    plt.title('Cross-Validation Scores')\n",
        "    plt.xlabel('Fold')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    cm = confusion_matrix(y_test, y_pred,labels=model.classes_)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=model.classes_)\n",
        "    disp.plot()\n",
        "    plt.xticks(fontsize=8)\n",
        "    plt.yticks(fontsize=8)\n",
        "\n",
        "    plt.show()\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAu0NVrlMmtX"
      },
      "source": [
        "Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SwrsPsTzMmtX"
      },
      "outputs": [],
      "source": [
        "def NB_classify(X_tr, y_tr, X_test, y_test, description):\n",
        "    model = MultinomialNB(alpha=0.1).fit(X_tr, y_tr)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    #Classification Report\n",
        "    clf_report = classification_report(y_test, y_pred, output_dict=True, zero_division=1)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.heatmap(pd.DataFrame(clf_report).iloc[:-1, :].T, annot=True)\n",
        "    plt.title(\"Naive Bayes Classification ({}) Accuracy Rate: {:.2f}%\".format(description, accuracy_score(y_test, y_pred)*100))\n",
        "    plt.show()\n",
        "\n",
        "    scores = cross_val_score(model, X_tr, y_tr, cv=5)\n",
        "    print(\"Accuracy: {:.2f}% (+/- {:.2f}%)\".format(scores.mean()*100, scores.std()*200))\n",
        "\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(scores, '-o', label='Cross-Validation Scores')\n",
        "    plt.axhline(y=scores.mean(), color='r', linestyle='--', label='Mean Score')\n",
        "    plt.title('Cross-Validation Scores')\n",
        "    plt.xlabel('Fold')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    cm = confusion_matrix(y_test, y_pred,labels=model.classes_)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=model.classes_)\n",
        "    disp.plot()\n",
        "    plt.xticks(fontsize=8)\n",
        "    plt.yticks(fontsize=8)\n",
        "\n",
        "    plt.show()\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9XV_5H9MmtX"
      },
      "source": [
        "### Multi Skill Classification (Single Label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S76PgqjaMmtX"
      },
      "outputs": [],
      "source": [
        "count_vect = CountVectorizer(ngram_range=(1, 2))\n",
        "tfidf_vectorizor = TfidfVectorizer(sublinear_tf=True, min_df=5,ngram_range=(1, 2),smooth_idf=True)\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(oneskillDF['description_cleaned'],oneskillDF['skill_abr_regroup'], test_size = 0.20, random_state = 60)\n",
        "\n",
        "X_train_counts = count_vect.fit_transform(X_train)\n",
        "X_train_tfidf = tfidf_vectorizor.fit_transform(X_train)\n",
        "\n",
        "X_test_counts = count_vect.transform(X_test)\n",
        "X_test_tfidf = tfidf_vectorizor.transform(X_test)\n",
        "\n",
        "print (\"Bag of Word Shape :\",X_train_counts.shape,X_test_counts.shape, Y_train.shape, Y_test.shape)\n",
        "print (\"TF-IDF Shape :\",X_test_counts.shape,X_test_tfidf.shape, Y_train.shape, Y_test.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tC3EV0PTMmtX"
      },
      "outputs": [],
      "source": [
        "#Logistic Regression\n",
        "one_LR_model_bow = LR_classify(X_train_counts, Y_train, X_test_counts, Y_test, 'Bag Of Words')\n",
        "one_LR_model_tfidf = LR_classify(X_train_tfidf, Y_train, X_test_tfidf, Y_test, 'TF-IDF')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LqnKNg3XMmtX"
      },
      "outputs": [],
      "source": [
        "#Naive Bayes\n",
        "one_NB_model_bow = NB_classify(X_train_counts, Y_train, X_test_counts, Y_test, 'Bag Of Words')\n",
        "one_NB_model_tfidf = NB_classify(X_train_tfidf, Y_train, X_test_tfidf, Y_test, 'TF-IDF')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ITx-XC3MmtX"
      },
      "source": [
        "### Multi Skill Classification (Single Label and Remove Unpredictable Skill within lag of data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQjHo4lHMmtX"
      },
      "source": [
        "decide to remove some skill that got low f1 score beacuse lag of data from previous model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKdwUbK9MmtX"
      },
      "outputs": [],
      "source": [
        "cutskillDF = oneskillDF[~oneskillDF['skill_abr_regroup'].isin(['PRDM', 'PROD', 'RSCH','DSGN','MRKT','EDU'])].copy()\n",
        "print(cutskillDF.shape)\n",
        "cutskillDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FULTcN4LMmtX"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "ax = sns.countplot(x=cutskillDF['skill_abr_regroup'], width=0.6)\n",
        "\n",
        "palette = sns.color_palette(\"deep\", len(ax.patches))\n",
        "for bar, color in zip(ax.patches, palette):\n",
        "    bar.set_color(color)\n",
        "\n",
        "for p in ax.patches:\n",
        "    ax.annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                ha='center', va='center', fontsize=10, color='black', xytext=(0, 5),\n",
        "                textcoords='offset points')\n",
        "\n",
        "nu_skill = cutskillDF['skill_abr_regroup'].nunique()\n",
        "ax.text(0.95, 0.95, f'Unique Skills : {nu_skill}', transform=ax.transAxes,\n",
        "        verticalalignment='top', horizontalalignment='right', fontsize=20, color='Black')\n",
        "\n",
        "jobs = cutskillDF['job_id'].nunique()\n",
        "ax.text(0.95, 0.85, f'Jobs : {jobs}', transform=ax.transAxes,\n",
        "        verticalalignment='top', horizontalalignment='right', fontsize=20, color='Black')\n",
        "\n",
        "plt.title('Count of Job Skills', fontsize=16)\n",
        "plt.xticks(rotation=45, fontsize=8)\n",
        "ax.set_xlabel(\"Skills\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p5lqI6sGMmtX"
      },
      "outputs": [],
      "source": [
        "cXtrain, cXtest, cY_trian, cY_test = train_test_split(cutskillDF['description_cleaned'],cutskillDF['skill_abr_regroup'], test_size = 0.20, random_state = 60)\n",
        "\n",
        "cXtrain_counts = count_vect.fit_transform(cXtrain)\n",
        "cXtrain_tfidf = tfidf_vectorizor.fit_transform(cXtrain)\n",
        "\n",
        "cXtest_counts = count_vect.transform(cXtest)\n",
        "cXtest_tfidf = tfidf_vectorizor.transform(cXtest)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fPQ2BzjMmtX"
      },
      "outputs": [],
      "source": [
        "#Logistic Regression\n",
        "cut_LR_model_bow = LR_classify(cXtrain_counts, cY_trian, cXtest_counts, cY_test, 'Bag Of Words')\n",
        "cut_LR_model_tfidf = LR_classify(cXtrain_tfidf, cY_trian, cXtest_tfidf, cY_test, 'TF-IDF')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FgprowChMmtX"
      },
      "outputs": [],
      "source": [
        "#Naive Bayes\n",
        "cut_NB_model_bow = NB_classify(cXtrain_counts, cY_trian, cXtest_counts, cY_test, 'Bag Of Words')\n",
        "cut_NB_model_tfidf = NB_classify(cXtrain_tfidf, cY_trian, cXtest_tfidf, cY_test, 'TF-IDF')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}